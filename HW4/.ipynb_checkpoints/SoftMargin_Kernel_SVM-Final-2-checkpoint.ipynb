{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Data Files\n",
    "#Features\n",
    "trainingData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "testData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testData.txt\", delimiter=\",\", unpack=False,dtype=np.float128)\n",
    "validationData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "\n",
    "#Labels\n",
    "trainingLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingLabels.txt\",unpack=False,dtype=np.float128)\n",
    "testLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testLabels.txt\",unpack=False,dtype=np.float128)\n",
    "validationLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationLabels.txt\",unpack=False,dtype=np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "trainingData_norm = (trainingData/128.0)-1.0\n",
    "testData_norm = (testData/128.0)-1.0\n",
    "validationData_norm = (validationData/128.0)-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate gradient\n",
    "def calculate_k(x,y):\n",
    "    return np.power((np.dot(x,y.T)/784+1),3)\n",
    "\n",
    "k=calculate_k(trainingData_norm,trainingData_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_val=calculate_k(validationData_norm,trainingData_norm)\n",
    "k_test=calculate_k(testData_norm,trainingData_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_a(a,k_a_b,lamda):\n",
    "    y=trainingLabels\n",
    "    N=len(y)\n",
    "    gradient=np.zeros((N,),dtype=np.float128)\n",
    "    for yp,p in zip(y,range(N)):\n",
    "        value=1-(yp*(k_a_b[p]))\n",
    "        if value>0:\n",
    "            gradient+=-1.0*(yp)*(k[p].T)\n",
    "            \n",
    "    gradient/=N\n",
    "    gradient=np.reshape(gradient,(N,1))\n",
    "    gradient+=((lamda/2.0)*np.dot((k+k.T),a))\n",
    "    return gradient\n",
    "\n",
    "def calculate_gradient_b(a,k_a_b,lamda):\n",
    "    y=trainingLabels\n",
    "    gradient=0\n",
    "    N=len(y)\n",
    "    for yp,p in zip(y,range(N)):\n",
    "        value=1-(yp*(k_a_b[p]))\n",
    "        if value>0:\n",
    "            gradient+=-1.0*yp\n",
    "    gradient/=N\n",
    "    return gradient\n",
    "\n",
    "#Main Function for Gradient Descent\n",
    "def gradient_descent(T,learning_rate,lamda):\n",
    "    a=np.zeros((len(trainingData_norm),1), dtype=np.float128)\n",
    "    b=0\n",
    "    for t in range(1,T+1):\n",
    "        k_a_b=np.dot(k,a)+b\n",
    "        gradient_a=calculate_gradient_a(a,k_a_b,lamda)\n",
    "        gradient_b=calculate_gradient_b(a,k_a_b,lamda)\n",
    "        a-=(learning_rate*gradient_a)\n",
    "        b-=(learning_rate*gradient_b)\n",
    "    return a,b\n",
    "\n",
    "#Function to calculate Error\n",
    "def calculate_error(a,b,x,y,k_):\n",
    "    x_train=trainingData_norm\n",
    "    N=len(y)\n",
    "    #k_=calculate_k(x,x_train)\n",
    "    prediction=np.sum(np.multiply(k_,a.T),axis=1)+b\n",
    "    prediction[prediction <= 0] = -1.0\n",
    "    prediction[prediction > 0] = 1.0\n",
    "    wrong_classification=np.sum(prediction != y)\n",
    "    error=wrong_classification*100.0/N\n",
    "    return error\n",
    "\n",
    "#Function to calculate Accuracy\n",
    "def calculate_accuracy(a,b,x,y,k_):\n",
    "    x_train=trainingData_norm\n",
    "    N=len(y)\n",
    "    #k_=calculate_k(x,x_train)\n",
    "    prediction=np.sum(np.multiply(k_,a.T),axis=1)+b\n",
    "    prediction[prediction <= 0] = -1.0\n",
    "    prediction[prediction > 0] = 1.0\n",
    "    correct_classification=np.sum(prediction == y)\n",
    "    accuracy=correct_classification*100.0/N\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation risk for greater than 54 (learning rates,Lambda) combinations\n",
      "learning_rate 1e-08 lamda 1e-10\n",
      "learning_rate 1e-08 lamda 1e-08\n",
      "learning_rate 1e-08 lamda 1e-05\n",
      "learning_rate 1e-08 lamda 0.01\n",
      "learning_rate 1e-08 lamda 1\n",
      "learning_rate 1e-08 lamda 10\n",
      "learning_rate 1e-07 lamda 1e-10\n",
      "learning_rate 1e-07 lamda 1e-08\n",
      "learning_rate 1e-07 lamda 1e-05\n",
      "learning_rate 1e-07 lamda 0.01\n",
      "learning_rate 1e-07 lamda 1\n",
      "learning_rate 1e-07 lamda 10\n",
      "learning_rate 1e-05 lamda 1e-10\n",
      "learning_rate 1e-05 lamda 1e-08\n",
      "learning_rate 1e-05 lamda 1e-05\n",
      "learning_rate 1e-05 lamda 0.01\n",
      "learning_rate 1e-05 lamda 1\n",
      "learning_rate 1e-05 lamda 10\n",
      "learning_rate 0.0001 lamda 1e-10\n",
      "learning_rate 0.0001 lamda 1e-08\n",
      "learning_rate 0.0001 lamda 1e-05\n",
      "learning_rate 0.0001 lamda 0.01\n",
      "learning_rate 0.0001 lamda 1\n",
      "learning_rate 0.0001 lamda 10\n",
      "learning_rate 0.001 lamda 1e-10\n",
      "learning_rate 0.001 lamda 1e-08\n",
      "learning_rate 0.001 lamda 1e-05\n",
      "learning_rate 0.001 lamda 0.01\n",
      "learning_rate 0.001 lamda 1\n",
      "learning_rate 0.001 lamda 10\n",
      "learning_rate 0.01 lamda 1e-10\n",
      "learning_rate 0.01 lamda 1e-08\n",
      "learning_rate 0.01 lamda 1e-05\n",
      "learning_rate 0.01 lamda 0.01\n",
      "learning_rate 0.01 lamda 1\n",
      "learning_rate 0.01 lamda 10\n",
      "learning_rate 1 lamda 1e-10\n",
      "learning_rate 1 lamda 1e-08\n",
      "learning_rate 1 lamda 1e-05\n",
      "learning_rate 1 lamda 0.01\n",
      "learning_rate 1 lamda 1\n",
      "learning_rate 1 lamda 10\n",
      "learning_rate 5 lamda 1e-10\n",
      "learning_rate 5 lamda 1e-08\n",
      "learning_rate 5 lamda 1e-05\n",
      "learning_rate 5 lamda 0.01\n",
      "learning_rate 5 lamda 1\n",
      "learning_rate 5 lamda 10\n",
      "learning_rate 10 lamda 1e-10\n",
      "learning_rate 10 lamda 1e-08\n",
      "learning_rate 10 lamda 1e-05\n",
      "learning_rate 10 lamda 0.01\n",
      "learning_rate 10 lamda 1\n",
      "learning_rate 10 lamda 10\n"
     ]
    }
   ],
   "source": [
    "#10 different Learning rates\n",
    "#Training and Validation risk for each learning rate\n",
    "print('Training and Validation risk for greater than 54 (learning rates,Lambda) combinations')\n",
    "validation_errors=[]\n",
    "training_errors=[]\n",
    "learning_rates=[]\n",
    "lambdas=[]\n",
    "T=500\n",
    "for learning_rate in [0.00000001,0.0000001,0.00001,0.0001,0.001,0.01,1,5,10]:\n",
    "    for lamda in [0.0000000001,0.00000001,0.00001,0.01,1,10]:\n",
    "        print('learning_rate',learning_rate,'lamda',lamda)\n",
    "        a,b=gradient_descent(T,learning_rate,lamda)\n",
    "        learning_rates.append(learning_rate)\n",
    "        lambdas.append(lamda)\n",
    "        \n",
    "        validation_error=calculate_error(a,b,validationData_norm,validationLabels)\n",
    "        validation_errors.append(validation_error)\n",
    "        \n",
    "        training_error=calculate_error(a,b,trainingData_norm,trainingLabels)\n",
    "        training_errors.append(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning_rate</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Training_Error</th>\n",
       "      <th>Validation_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>48.698215</td>\n",
       "      <td>48.714590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.002620</td>\n",
       "      <td>10.987392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>12.002620</td>\n",
       "      <td>10.987392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>12.002620</td>\n",
       "      <td>11.003766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>12.002620</td>\n",
       "      <td>11.118389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>40.248895</td>\n",
       "      <td>39.970526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>40.265269</td>\n",
       "      <td>40.363517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>40.281644</td>\n",
       "      <td>40.363517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>40.265269</td>\n",
       "      <td>40.363517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>48.059604</td>\n",
       "      <td>48.141477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>36.908466</td>\n",
       "      <td>36.302604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>36.793843</td>\n",
       "      <td>36.269854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>37.235959</td>\n",
       "      <td>36.630097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>35.041755</td>\n",
       "      <td>34.403144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>35.041755</td>\n",
       "      <td>34.419519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>37.972818</td>\n",
       "      <td>37.465204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>35.074505</td>\n",
       "      <td>34.403144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>35.140003</td>\n",
       "      <td>34.534141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>35.074505</td>\n",
       "      <td>34.452268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>35.762240</td>\n",
       "      <td>35.221877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>35.074505</td>\n",
       "      <td>34.452268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>37.727198</td>\n",
       "      <td>37.252333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>51.301785</td>\n",
       "      <td>51.285410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(columns=['Learning_rate','Lambda','Training_Error','Validation_Error'])\n",
    "df['Learning_rate']=learning_rates\n",
    "df['Lambda']=lambdas\n",
    "df['Training_Error']=training_errors\n",
    "df['Validation_Error']=validation_errors\n",
    "HTML(df.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate for least validation risk 1e-05\n",
      "lambda for least validation risk 1e-10\n",
      "Validation Error for learning rate: 1e-05 and lambda: 1e-10 is: 10.9873915179\n"
     ]
    }
   ],
   "source": [
    "#Learning_rate which yields least validation_error\n",
    "minimum_validation_error=min(validation_errors)\n",
    "index_of_minimum=validation_errors.index(minimum_validation_error)\n",
    "learning_rate,lamda=learning_rates[index_of_minimum],lambdas[index_of_minimum]\n",
    "print(\"learning_rate for least validation risk\",learning_rate)\n",
    "print(\"lambda for least validation risk\",lamda)\n",
    "print('Validation Error for learning rate:',learning_rate,'and lambda:',lamda,'is:',minimum_validation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to generate list of empirical risk for iterations 1 to T\n",
    "def Empiricial_risk_vs_iterations(T,learning_rate,lamda):\n",
    "    a=np.zeros((len(trainingData_norm),1), dtype=np.float128)\n",
    "    b=0\n",
    "    list_empirical_risk=[]\n",
    "    for t in range(1,T+1):\n",
    "        k_a_b=np.dot(k,a)+b\n",
    "        gradient_a=calculate_gradient_a(a,k_a_b,lamda)\n",
    "        gradient_b=calculate_gradient_b(a,k_a_b,lamda)\n",
    "        a-=(learning_rate*gradient_a)\n",
    "        b-=(learning_rate*gradient_b)\n",
    "        \n",
    "        empirical_risk=calculate_error(a,b,trainingData_norm,trainingLabels,k)\n",
    "        list_empirical_risk.append(empirical_risk)\n",
    "    return a,b,list_empirical_risk\n",
    "\n",
    "#list of empirical risk values, for iterations from 1 to 1000\n",
    "T=500\n",
    "a,b,list_empirical_risk=Empiricial_risk_vs_iterations(T,learning_rate,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAJcCAYAAACxEXM4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntKhEFEvxIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117dccbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note:\n",
      "Empirical Risk reduces as iterations increase as expected\n"
     ]
    }
   ],
   "source": [
    "#Plot Empirical Risk vs Iterations\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,501), list_empirical_risk, 'r--')\n",
    "plt.ylabel('Empiricial risk')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(' Empirical Risk vs Iterations')\n",
    "plt.show()\n",
    "\n",
    "print('\\nNote:')\n",
    "print('Empirical Risk reduces as iterations increase as expected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: learning rate: 1e-05 and lamda: 1e-10  for iterations: 500\n",
      "Test Error of the best model: 9.62199312715\n",
      "\n",
      "Note:\n",
      "Test Risk: 9.62199312715 and Validation Risk: 10.9873915179 are close\n",
      "\n",
      "validation_accuracy 89.0126084821\n",
      "test_accuracy 90.3780068729\n",
      "\n",
      "Note:\n",
      "Validation Accuracy and Test Accuracy are close\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "#Best model using the learning rate, lambda found\n",
    "#a,b=gradient_descent(number_of_iterations,learning_rate,lamda)\n",
    "test_error=calculate_error(a,b,testData_norm,testLabels,k_test)\n",
    "print(\"best model: learning rate:\",learning_rate,'and lamda:',lamda,' for iterations:',500)\n",
    "print('Test Error of the best model:',test_error)\n",
    "\n",
    "print('\\nNote:')\n",
    "print('Test Risk:',test_error, 'and Validation Risk:',minimum_validation_error,'are close')\n",
    "\n",
    "#Validation Accuracy vs Test Accuracy\n",
    "test_accuracy=calculate_accuracy(a,b,testData_norm,testLabels,k_test)\n",
    "validation_accuracy=calculate_accuracy(a,b,validationData_norm,validationLabels,k_val)\n",
    "\n",
    "print('\\nvalidation_accuracy',validation_accuracy)\n",
    "print('test_accuracy',test_accuracy)\n",
    "\n",
    "print('\\nNote:')\n",
    "print('Validation Accuracy and Test Accuracy are close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
