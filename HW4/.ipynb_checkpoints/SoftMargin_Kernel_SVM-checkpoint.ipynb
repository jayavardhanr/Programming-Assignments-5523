{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "np.seterr(all='warn')\n",
    "\n",
    "#Loading Data Files\n",
    "#Features\n",
    "trainingData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "testData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testData.txt\", delimiter=\",\", unpack=False,dtype=np.float128)\n",
    "validationData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "\n",
    "#Labels\n",
    "trainingLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingLabels.txt\",unpack=False,dtype=np.float128)\n",
    "testLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testLabels.txt\",unpack=False,dtype=np.float128)\n",
    "validationLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationLabels.txt\",unpack=False,dtype=np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate gradient\n",
    "def calculate_k(x,y):\n",
    "    k=np.power((np.dot(x,y.T)+1),3)\n",
    "    return np.clip(k,np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "\n",
    "k=calculate_k(trainingData,trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_a(a,k_a_b,lamda):\n",
    "    y=trainingLabels\n",
    "    N=len(y)\n",
    "    gradient=np.zeros((N,),dtype=np.float128)\n",
    "    for yp,p in zip(y,range(N)):\n",
    "        value=1-(yp*(k_a_b[p]))\n",
    "        if value>0:\n",
    "            gradient+=-1.0*(yp)*(k[p].T)\n",
    "            \n",
    "    gradient=np.clip(gradient,np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "    gradient/=N\n",
    "    gradient=np.reshape(gradient,(N,1))\n",
    "    gradient+=np.clip(((lamda/2.0)*np.dot((k+k.T),a)),np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "    return np.clip(gradient,np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "\n",
    "def calculate_gradient_b(a,k_a_b,lamda):\n",
    "    y=trainingLabels\n",
    "    gradient=0\n",
    "    N=len(y)\n",
    "    for yp,p in zip(y,range(N)):\n",
    "        value=1-(yp*(k_a_b[p]))\n",
    "        if value>0:\n",
    "            gradient+=-1.0*yp\n",
    "    gradient/=N\n",
    "    return gradient\n",
    "\n",
    "#Main Function for Gradient Descent\n",
    "def gradient_descent(T,learning_rate,lamda):\n",
    "    a=np.zeros((len(trainingData),1), dtype=np.float128)\n",
    "    b=0\n",
    "    for t in range(1,T+1):\n",
    "        k_a=np.clip(np.dot(k,a),np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "        k_a_b=np.clip(k_a+b,np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "        gradient_a=calculate_gradient_a(a,k_a_b,lamda)\n",
    "        gradient_b=calculate_gradient_b(a,k_a_b,lamda)\n",
    "        a-=(learning_rate*gradient_a)\n",
    "        b-=(learning_rate*gradient_b)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.7976931e+303]\n",
      " [ 1.7976931e+303]\n",
      " [ 1.7976931e+303]\n",
      " ..., \n",
      " [ 1.7976931e+303]\n",
      " [ 1.7976931e+303]\n",
      " [ 1.7976931e+303]]\n",
      "6.78041591616e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.00001\n",
    "lamda=0.001\n",
    "T=500\n",
    "a,b=gradient_descent(T,learning_rate,lamda)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  9]\n",
      " [ 4 10 18]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_=np.array([[1,2,3],[4,5,6]])\n",
    "b_=np.array([1,2,3])\n",
    "c_=np.multiply(a_,b_)\n",
    "print(c_)\n",
    "su=np.sum(c_,axis=1)\n",
    "su.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate Risk\n",
    "def calculate_error(a,b,x,y):\n",
    "    x_train=trainingData\n",
    "    N=len(y)\n",
    "    k_=calculate_k(x,x_train)\n",
    "    prediction=np.sum(np.multiply(k_,a),axis=1)+b\n",
    "    prediction[prediction <= 0] = -1.0\n",
    "    prediction[prediction > 0] = 1.0\n",
    "    wrong_classification=len([a for a,b in zip(prediction,y) if a!=b])\n",
    "    print(wrong_classification)\n",
    "    error=wrong_classification*100.0/N\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 0.001\n",
      "2974\n",
      "48.69821516292779\n"
     ]
    }
   ],
   "source": [
    "print('calculating Validation risk for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "print(calculate_error(a,b,trainingData,trainingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#10 different Learning rates\n",
    "#Training and Validation risk for each learning rate\n",
    "print('Training and Validation risk for 54 (learning rates,Lambda) combinations')\n",
    "print('Learning rates:0.000001,0.000005,0.00001,0.00005,0.0001,0.005,0.001,0.01,1')\n",
    "print('Lambda:0.0001,0.001,0.01,0.1,0,1,10')\n",
    "print('\\n')\n",
    "\n",
    "validation_risk_dictionary={}\n",
    "training_risk_dictionary={}\n",
    "T=500\n",
    "for learning_rate in [0.000001,0.000005,0.00001,0.00005,0.0001,0.005,0.001,0.01,1]:\n",
    "    for lamda in [0.0001,0.001,0.01,0.1,0,1,10]:\n",
    "        \n",
    "        print('Gradient Descent for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "        w=gradient_descent(T,learning_rate,lamda)\n",
    "        \n",
    "        print('calculating Validation risk for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "        validation_risk=calculate_risk(w,validationData,validationLabels,lamda)\n",
    "        validation_risk_dictionary[(learning_rate,lamda)]=validation_risk\n",
    "        \n",
    "        print('calculating Training risk for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "        training_risk=calculate_risk(w,trainingData,trainingLabels,lamda)\n",
    "        training_risk_dictionary[(learning_rate,lamda)]=training_risk\n",
    "        print('\\n')\n",
    "\n",
    "print('\\n')\n",
    "print('Training Risk for 54 Combinations of Learning rate and Lambda')\n",
    "print(training_risk_dictionary)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Validation Risk for 54 Combinations of Learning rate and Lambda')\n",
    "print(validation_risk_dictionary)\n",
    "print('\\n\\n')\n",
    "\n",
    "#Learning_rate which yields least validation_error\n",
    "learning_rate,lamda=min(validation_risk_dictionary, key=validation_risk_dictionary.get)\n",
    "print(\"learning_rate for least validation risk\",learning_rate)\n",
    "print(\"lambda for least validation risk\",lamda)\n",
    "print('validation risk for learning rate:',learning_rate,'and lambda:',lamda,'is:',validation_risk_dictionary[(learning_rate,lamda)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function used to generate list of empirical risk for iterations 1 to 1000\n",
    "def Empiricial_risk_vs_iterations(T,learning_rate):\n",
    "    w=np.zeros((1,784), dtype=np.float128)\n",
    "    list_empirical_risk=[]\n",
    "    for t in range(1,T+1):\n",
    "        gradient=calculate_gradient(w)\n",
    "        w-=(learning_rate*gradient)\n",
    "        empirical_risk=calculate_risk(w,trainingData,trainingLabels)\n",
    "        list_empirical_risk.append(empirical_risk)\n",
    "    return list_empirical_risk\n",
    "\n",
    "#list of empirical risk values, for iterations from 1 to 1000\n",
    "T=1000\n",
    "list_empirical_risk=Empiricial_risk_vs_iterations(T,learning_rate)\n",
    "\n",
    "#Plot Empirical Risk vs Iterations\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,1001), list_empirical_risk, 'r--')\n",
    "plt.ylabel('Empiricial risk')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(' Empirical Risk vs Iterations')\n",
    "plt.show()\n",
    "\n",
    "#Find number of iterations which gives minimum risk \n",
    "number_of_iterations=list_empirical_risk.index(min(list_empirical_risk))+1\n",
    "print(\"Iteration which produces least empirical risk:\",number_of_iterations)\n",
    "\n",
    "#Final Model\n",
    "#Best model using the learning rate and number_of_iterations found\n",
    "w=gradient_descent(number_of_iterations,learning_rate)\n",
    "test_risk=calculate_risk(w,testData,testLabels)\n",
    "print(\"best model: learning rate:\",learning_rate,'iterations:',number_of_iterations)\n",
    "print('Test Risk of the best model:',test_risk)\n",
    "\n",
    "print('\\nNote:')\n",
    "print('Test Risk:',test_risk, 'and Validation error:', validation_risk_dictionary[learning_rate],'are close')\n",
    "print('Empirical Risk reduces as iterations increase as expected')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
