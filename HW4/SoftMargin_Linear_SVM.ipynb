{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading Data Files\n",
    "#Features\n",
    "trainingData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "testData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testData.txt\", delimiter=\",\", unpack=False,dtype=np.float128)\n",
    "validationData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "\n",
    "#Labels\n",
    "trainingLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingLabels.txt\",unpack=False)\n",
    "testLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testLabels.txt\",unpack=False)\n",
    "validationLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationLabels.txt\",unpack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate gradient\n",
    "def calculate_gradient(w,lamda):\n",
    "    x=trainingData\n",
    "    y=trainingLabels\n",
    "    gradient=0\n",
    "    N=len(y)\n",
    "    for xi,yi in zip(x,y):\n",
    "        product=yi*np.dot(w,np.transpose(xi))\n",
    "        if product<1:\n",
    "            gradient+=((-1*yi)/N)*(xi).T\n",
    "    gradient=np.reshape(gradient,(1,784))\n",
    "    gradient+=lamda*w\n",
    "    return gradient\n",
    "\n",
    "#Main Function for Gradient Descent\n",
    "def gradient_descent(T,learning_rate,lamda):\n",
    "    w=np.zeros((1,784), dtype=np.float128)\n",
    "    for t in range(1,T+1):\n",
    "        gradient=calculate_gradient(w,lamda)\n",
    "        w-=(learning_rate*gradient)\n",
    "    return w\n",
    "\n",
    "#Function to calculate Risk\n",
    "def calculate_risk(w,x,y,lamda):\n",
    "    predictions=np.dot(x,np.transpose(w))\n",
    "    N=len(y)\n",
    "    y=y.reshape((N,1))\n",
    "    values=1-np.multiply(y,predictions)\n",
    "    risk=np.sum(values.clip(0))/N + lamda*np.sum(np.square(w))\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation risk for 54 (learning rates,Lambda) combinations\n",
      "Learning rates:0.000001,0.000005,0.00001,0.00005,0.0001,0.005,0.001,0.01,1\n",
      "Lambda:0.0001,0.001,0.01,0.1,0,1,10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-06 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 1e-06 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 1e-06 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-06 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 1e-06 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 1e-06 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-06 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 1e-06 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 1e-06 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-06 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 1e-06 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 1e-06 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-06 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 1e-06 and lamda: 0\n",
      "calculating Training risk for learning_rate: 1e-06 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-06 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 1e-06 and lamda: 1\n",
      "calculating Training risk for learning_rate: 1e-06 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-06 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 1e-06 and lamda: 10\n",
      "calculating Training risk for learning_rate: 1e-06 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-06 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 5e-06 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 5e-06 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-06 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 5e-06 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 5e-06 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-06 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 5e-06 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 5e-06 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-06 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 5e-06 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 5e-06 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-06 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 5e-06 and lamda: 0\n",
      "calculating Training risk for learning_rate: 5e-06 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-06 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 5e-06 and lamda: 1\n",
      "calculating Training risk for learning_rate: 5e-06 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-06 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 5e-06 and lamda: 10\n",
      "calculating Training risk for learning_rate: 5e-06 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-05 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 1e-05 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-05 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 1e-05 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-05 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 1e-05 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-05 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 1e-05 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-05 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 0\n",
      "calculating Training risk for learning_rate: 1e-05 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-05 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 1\n",
      "calculating Training risk for learning_rate: 1e-05 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1e-05 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 10\n",
      "calculating Training risk for learning_rate: 1e-05 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-05 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 5e-05 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 5e-05 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-05 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 5e-05 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 5e-05 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-05 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 5e-05 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 5e-05 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-05 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 5e-05 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 5e-05 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-05 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 5e-05 and lamda: 0\n",
      "calculating Training risk for learning_rate: 5e-05 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-05 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 5e-05 and lamda: 1\n",
      "calculating Training risk for learning_rate: 5e-05 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 5e-05 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 5e-05 and lamda: 10\n",
      "calculating Training risk for learning_rate: 5e-05 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.0001 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 0.0001 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 0.0001 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.0001 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 0.0001 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 0.0001 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.0001 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 0.0001 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 0.0001 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.0001 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 0.0001 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 0.0001 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.0001 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 0.0001 and lamda: 0\n",
      "calculating Training risk for learning_rate: 0.0001 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.0001 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 0.0001 and lamda: 1\n",
      "calculating Training risk for learning_rate: 0.0001 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.0001 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 0.0001 and lamda: 10\n",
      "calculating Training risk for learning_rate: 0.0001 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.005 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 0.005 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 0.005 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.005 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 0.005 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 0.005 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.005 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 0.005 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 0.005 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.005 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 0.005 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 0.005 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.005 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 0.005 and lamda: 0\n",
      "calculating Training risk for learning_rate: 0.005 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.005 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 0.005 and lamda: 1\n",
      "calculating Training risk for learning_rate: 0.005 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.005 and lamda: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating Validation risk for learning_rate: 0.005 and lamda: 10\n",
      "calculating Training risk for learning_rate: 0.005 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.001 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 0.001 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 0.001 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.001 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 0.001 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 0.001 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.001 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 0.001 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 0.001 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.001 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 0.001 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 0.001 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.001 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 0.001 and lamda: 0\n",
      "calculating Training risk for learning_rate: 0.001 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.001 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 0.001 and lamda: 1\n",
      "calculating Training risk for learning_rate: 0.001 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.001 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 0.001 and lamda: 10\n",
      "calculating Training risk for learning_rate: 0.001 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.01 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 0.01 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 0.01 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.01 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 0.01 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 0.01 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.01 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 0.01 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 0.01 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.01 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 0.01 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 0.01 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.01 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 0.01 and lamda: 0\n",
      "calculating Training risk for learning_rate: 0.01 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.01 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 0.01 and lamda: 1\n",
      "calculating Training risk for learning_rate: 0.01 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 0.01 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 0.01 and lamda: 10\n",
      "calculating Training risk for learning_rate: 0.01 and lamda: 10\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1 and lamda: 0.0001\n",
      "calculating Validation risk for learning_rate: 1 and lamda: 0.0001\n",
      "calculating Training risk for learning_rate: 1 and lamda: 0.0001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1 and lamda: 0.001\n",
      "calculating Validation risk for learning_rate: 1 and lamda: 0.001\n",
      "calculating Training risk for learning_rate: 1 and lamda: 0.001\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1 and lamda: 0.01\n",
      "calculating Validation risk for learning_rate: 1 and lamda: 0.01\n",
      "calculating Training risk for learning_rate: 1 and lamda: 0.01\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1 and lamda: 0.1\n",
      "calculating Validation risk for learning_rate: 1 and lamda: 0.1\n",
      "calculating Training risk for learning_rate: 1 and lamda: 0.1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1 and lamda: 0\n",
      "calculating Validation risk for learning_rate: 1 and lamda: 0\n",
      "calculating Training risk for learning_rate: 1 and lamda: 0\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1 and lamda: 1\n",
      "calculating Validation risk for learning_rate: 1 and lamda: 1\n",
      "calculating Training risk for learning_rate: 1 and lamda: 1\n",
      "\n",
      "\n",
      "Gradient Descent for learning_rate: 1 and lamda: 10\n",
      "calculating Validation risk for learning_rate: 1 and lamda: 10\n",
      "calculating Training risk for learning_rate: 1 and lamda: 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training Risk for 54 Combinations of Learning rate and Lambda\n",
      "{(1e-05, 0): 0.12935597814307048801, (5e-05, 1): 0.35652317558178080214, (0.001, 0): 2.0473980686222602889, (1, 0.01): 5151.1521452016529228, (0.01, 0.1): 55.425750273958063773, (1e-05, 1): 0.091922846191504190053, (0.01, 0.01): 21.852676286054883723, (1e-06, 10): 0.11416369020708797224, (1e-05, 10): 0.093960838753417102601, (5e-06, 0.1): 0.092936678802669513589, (1e-06, 1): 0.11355583616204355687, (0.0001, 1): 0.23211473011529918421, (0.01, 10): 3239.1662321028423388, (1, 0.1): 12513.163346531773678, (0.005, 10): 45.237089390231677161, (1, 0.001): 5602.8615446186759224, (1, 0): 2254.0491352979825475, (1e-05, 0.0001): 0.12935602970574400606, (0.001, 1): 4.7176951881055508463, (5e-06, 0.0001): 0.094476497092527514425, (1, 0.0001): 2098.7712045901540425, (5e-05, 0.1): 0.33401482894774985605, (1e-05, 0.1): 0.12814946038377738614, (0.01, 0.0001): 19.533411509790386909, (1e-06, 0.01): 0.11348988490351241097, (1e-06, 0): 0.11348870624611598022, (0.0001, 0): 0.24338200567313189567, (0.001, 10): 229.09151286202216814, (0.001, 0.001): 2.1660657275338688922, (1, 10): 3.1681937983516666389e+958, (5e-05, 10): 0.24407816323749319255, (0.001, 0.0001): 2.0506565227811678809, (5e-05, 0): 0.38582479076200427339, (0.005, 0): 10.684310997435640138, (0.001, 0.01): 2.0203186680795181356, (0.005, 0.0001): 10.38731009840743853, (0.0001, 0.001): 0.24338598461328130337, (0.001, 0.1): 2.2396740417385048155, (0.01, 0): 21.100363074862031804, (1e-06, 0.001): 0.11348877736929743458, (5e-05, 0.0001): 0.3858251269938352624, (0.0001, 0.0001): 0.24338240365712953157, (5e-06, 0): 0.09447647694374184608, (1e-05, 0.001): 0.12782644881171490104, (5e-05, 0.001): 0.33921510844506511619, (0.005, 1): 60.750002930574445975, (0.0001, 0.01): 0.22365038651203566133, (0.0001, 10): 0.42141977099903356121, (5e-06, 0.01): 0.09285143628624965766, (0.01, 1): 44.203967467642435757, (1e-05, 0.01): 0.090794904782313007778, (0.005, 0.01): 10.5045993963865933, (5e-05, 0.01): 0.34773611384134027634, (5e-06, 1): 0.093751724254011762896, (5e-06, 0.001): 0.093064755885838207302, (1e-06, 0.1): 0.11349447218600595043, (1, 1): 1280566.3093753075464, (5e-06, 10): 0.096756452347325807912, (0.0001, 0.1): 0.26375592881853987434, (0.01, 0.001): 20.920979171026703562, (1e-06, 0.0001): 0.11348871335843856801, (0.005, 0.1): 15.471858796804241049, (0.005, 0.001): 11.119220902292244212}\n",
      "\n",
      "\n",
      "\n",
      "Validation Risk for 54 Combinations of Learning rate and Lambda\n",
      "{(1e-05, 0): 0.14839438160001774416, (5e-05, 1): 0.43847286557358287193, (0.001, 0): 3.4016602376606329078, (1, 0.01): 5706.1384919884891991, (0.01, 0.1): 69.45672616769025285, (1e-05, 1): 0.11881707518925084874, (0.01, 0.01): 34.793674631768141226, (1e-06, 10): 0.12301820197398840367, (1e-05, 10): 0.12033201106452520302, (5e-06, 0.1): 0.11167285692576966181, (1e-06, 1): 0.12242718608867482672, (0.0001, 1): 0.37230680085945708224, (0.01, 10): 3206.1354316137235099, (1, 0.1): 12694.181832968845279, (0.005, 10): 46.759896046646847137, (1, 0.001): 7012.336605771367382, (1, 0): 3683.8131905316386447, (1e-05, 0.0001): 0.14839442491169178461, (0.001, 1): 6.1010359309904749728, (5e-06, 0.0001): 0.11328563809705385031, (1, 0.0001): 3400.4023026745080389, (5e-05, 0.1): 0.42014570623341722773, (1e-05, 0.1): 0.14846180245183107792, (0.01, 0.0001): 32.533960001618506788, (1e-06, 0.01): 0.12236458968871124816, (1e-06, 0): 0.12236206059819791929, (0.0001, 0): 0.38782855972051715463, (0.001, 10): 228.99969441239902299, (0.001, 0.001): 3.5146245392017437697, (1, 10): 3.1681937983516666389e+958, (5e-05, 10): 0.33163168153225673615, (0.001, 0.0001): 3.4062651315261797398, (5e-05, 0): 0.46398074008574080501, (0.005, 0): 17.611848145198629224, (0.001, 0.01): 3.3661588525967436262, (0.005, 0.0001): 17.166882900742264218, (0.0001, 0.001): 0.38782622678378740806, (0.001, 0.1): 3.5835838955168668183, (0.01, 0): 34.883715758188087175, (1e-06, 0.001): 0.12236213034558287345, (5e-05, 0.0001): 0.46398091707598074196, (0.0001, 0.0001): 0.38782832650396943807, (5e-06, 0): 0.11328562117284068571, (1e-05, 0.001): 0.14729253939494322809, (5e-05, 0.001): 0.42396041910360600743, (0.005, 1): 65.807123638776776106, (0.0001, 0.01): 0.3668203034982983877, (0.0001, 10): 0.56218190547441656548, (5e-06, 0.01): 0.11161447870644834768, (0.01, 1): 49.330990498101115846, (1e-05, 0.01): 0.11749993431963633173, (0.005, 0.01): 17.058122219793048718, (5e-05, 0.01): 0.43195048116326445229, (5e-06, 1): 0.11262799716113288906, (5e-06, 0.001): 0.11194954077522240329, (1e-06, 0.1): 0.12236610309559022082, (1, 1): 1275150.533101076339, (5e-06, 10): 0.11386508532799280559, (0.0001, 0.1): 0.39889569391089898368, (0.01, 0.001): 34.241524541414687929, (1e-06, 0.0001): 0.1223620675729408427, (0.005, 0.1): 20.008263837495967735, (0.005, 0.001): 18.178538735095031443}\n",
      "\n",
      "\n",
      "\n",
      "learning_rate for least validation risk 5e-06\n",
      "lambda for least validation risk 0.01\n",
      "validation risk for learning rate: 5e-06 and lambda: 0.01 is: 0.111614478706\n"
     ]
    }
   ],
   "source": [
    "#10 different Learning rates\n",
    "#Training and Validation risk for each learning rate\n",
    "print('Training and Validation risk for 63 (learning rates,Lambda) combinations')\n",
    "print('Learning rates:0.000001,0.000005,0.00001,0.00005,0.0001,0.005,0.001,0.01,1')\n",
    "print('Lambda:0.0001,0.001,0.01,0.1,0,1,10')\n",
    "print('\\n')\n",
    "\n",
    "validation_risk_dictionary={}\n",
    "training_risk_dictionary={}\n",
    "T=500\n",
    "for learning_rate in [0.000001,0.000005,0.00001,0.00005,0.0001,0.005,0.001,0.01,1]:\n",
    "    for lamda in [0.0001,0.001,0.01,0.1,0,1,10]:\n",
    "        \n",
    "        print('Gradient Descent for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "        w=gradient_descent(T,learning_rate,lamda)\n",
    "        \n",
    "        print('calculating Validation risk for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "        validation_risk=calculate_risk(w,validationData,validationLabels,lamda)\n",
    "        validation_risk_dictionary[(learning_rate,lamda)]=validation_risk\n",
    "        \n",
    "        print('calculating Training risk for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "        training_risk=calculate_risk(w,trainingData,trainingLabels,lamda)\n",
    "        training_risk_dictionary[(learning_rate,lamda)]=training_risk\n",
    "        print('\\n')\n",
    "\n",
    "print('\\n')\n",
    "print('Training Risk for 63 Combinations of Learning rate and Lambda')\n",
    "print(training_risk_dictionary)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Validation Risk for 63 Combinations of Learning rate and Lambda')\n",
    "print(validation_risk_dictionary)\n",
    "print('\\n\\n')\n",
    "\n",
    "#Learning_rate which yields least validation_error\n",
    "learning_rate,lamda=min(validation_risk_dictionary, key=validation_risk_dictionary.get)\n",
    "print(\"learning_rate for least validation risk\",learning_rate)\n",
    "print(\"lambda for least validation risk\",lamda)\n",
    "print('validation risk for learning rate:',learning_rate,'and lambda:',lamda,'is:',validation_risk_dictionary[(learning_rate,lamda)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function used to generate list of empirical risk for iterations 1 to T\n",
    "def Empiricial_risk_vs_iterations(T,learning_rate,lamda):\n",
    "    w=np.zeros((1,784), dtype=np.float128)\n",
    "    list_empirical_risk=[]\n",
    "    for t in range(1,T+1):\n",
    "        gradient=calculate_gradient(w,lamda)\n",
    "        w-=(learning_rate*gradient)\n",
    "        empirical_risk=calculate_risk(w,trainingData,trainingLabels,lamda)\n",
    "        list_empirical_risk.append(empirical_risk)\n",
    "    return list_empirical_risk\n",
    "\n",
    "#list of empirical risk values, for iterations from 1 to 1000\n",
    "T=500\n",
    "list_empirical_risk=Empiricial_risk_vs_iterations(T,learning_rate,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list_empirical_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAJcCAYAAABXOLh8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRGVJyCIiIiIq+xvXc9geCK11LAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ca4ab70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration which produces least empirical risk: 500\n"
     ]
    }
   ],
   "source": [
    "#Plot Empirical Risk vs Iterations\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,501), list_empirical_risk, 'r--')\n",
    "plt.ylabel('Empiricial risk')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(' Empirical Risk vs Iterations')\n",
    "plt.show()\n",
    "\n",
    "#Find number of iterations which gives minimum risk \n",
    "number_of_iterations=list_empirical_risk.index(min(list_empirical_risk))+1\n",
    "print(\"Iteration which produces least empirical risk:\",number_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: learning rate: 5e-06 and lamda: 0.01  for iterations: 500\n",
      "Test Risk of the best model: 0.106093883233\n",
      "\n",
      "Note:\n",
      "Test Risk: 0.106093883233 and Validation error: 0.111614478706 are close\n",
      "Empirical Risk reduces as iterations increase as expected\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "#Best model using the learning rate and number_of_iterations found\n",
    "w=gradient_descent(number_of_iterations,learning_rate,lamda)\n",
    "test_risk=calculate_risk(w,testData,testLabels,lamda)\n",
    "print(\"best model: learning rate:\",learning_rate,'and lamda:',lamda,' for iterations:',number_of_iterations)\n",
    "print('Test Risk of the best model:',test_risk)\n",
    "\n",
    "print('\\nNote:')\n",
    "print('Test Risk:',test_risk, 'and Validation error:', validation_risk_dictionary[(learning_rate,lamda)],'are close')\n",
    "print('Empirical Risk reduces as iterations increase as expected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
