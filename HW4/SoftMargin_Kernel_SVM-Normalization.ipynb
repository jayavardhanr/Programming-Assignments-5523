{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Data Files\n",
    "#Features\n",
    "trainingData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "testData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testData.txt\", delimiter=\",\", unpack=False,dtype=np.float128)\n",
    "validationData = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationData.txt\", delimiter=\",\",unpack=False,dtype=np.float128)\n",
    "\n",
    "#Labels\n",
    "trainingLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/trainingLabels.txt\",unpack=False,dtype=np.float128)\n",
    "testLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/testLabels.txt\",unpack=False,dtype=np.float128)\n",
    "validationLabels = loadtxt(\"/Users/jayavardhanreddy/Python_Code/ML/HW4/DataFiles/validationLabels.txt\",unpack=False,dtype=np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "trainingData_norm = (trainingData/128.0)-1.0\n",
    "testData_norm = (testData/128.0)-1.0\n",
    "validationData_norm = (validationData/128.0)-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate gradient\n",
    "def calculate_k(x,y):\n",
    "    return np.power((np.dot(x,y.T)/784+1),3)\n",
    "\n",
    "k=calculate_k(trainingData_norm,trainingData_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_a(a,k_a_b,lamda):\n",
    "    y=trainingLabels\n",
    "    N=len(y)\n",
    "    gradient=np.zeros((N,),dtype=np.float128)\n",
    "    for yp,p in zip(y,range(N)):\n",
    "        value=1-(yp*(k_a_b[p]))\n",
    "        if value>=0:\n",
    "            gradient+=-1.0*(yp)*(k[p].T)\n",
    "            \n",
    "    gradient/=N\n",
    "    gradient=np.reshape(gradient,(N,1))\n",
    "    gradient+=((lamda/2.0)*np.dot((k+k.T),a))\n",
    "    return gradient\n",
    "\n",
    "def calculate_gradient_b(a,k_a_b,lamda):\n",
    "    y=trainingLabels\n",
    "    gradient=0\n",
    "    N=len(y)\n",
    "    for yp,p in zip(y,range(N)):\n",
    "        value=1-(yp*(k_a_b[p]))\n",
    "        if value>=0:\n",
    "            gradient+=-1.0*yp\n",
    "    gradient/=N\n",
    "    return gradient\n",
    "\n",
    "#Main Function for Gradient Descent\n",
    "def gradient_descent(T,learning_rate,lamda):\n",
    "    a=np.zeros((len(trainingData_norm),1), dtype=np.float128)\n",
    "    b=0\n",
    "    for t in range(1,T+1):\n",
    "        k_a_b=np.dot(k,a)+b\n",
    "        gradient_a=calculate_gradient_a(a,k_a_b,lamda)\n",
    "        gradient_b=calculate_gradient_b(a,k_a_b,lamda)\n",
    "        a-=(learning_rate*gradient_a)\n",
    "        b-=(learning_rate*gradient_b)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.9972937e-05]\n",
      " [ 0.00012790565]\n",
      " [ 0.00075335095]\n",
      " ..., \n",
      " [ 0.00034678965]\n",
      " [-0.00046754469]\n",
      " [-0.00068844037]]\n",
      "-3.08482069756e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.00001\n",
    "lamda=0.001\n",
    "T=500\n",
    "a,b=gradient_descent(T,learning_rate,lamda)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate Risk\n",
    "def calculate_error(a,b,x,y):\n",
    "    x_train=trainingData_norm\n",
    "    N=len(y)\n",
    "    k_=calculate_k_linear(x,x_train)\n",
    "    prediction=np.sum(np.multiply(k_,a),axis=1)+b\n",
    "    prediction[prediction <= 0] = -1.0\n",
    "    prediction[prediction > 0] = 1.0\n",
    "    wrong_classification=len([a for a,b in zip(prediction,y) if a!=b])\n",
    "    error=wrong_classification*100.0/N\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating Validation risk for learning_rate: 1e-05 and lamda: 0.001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_k_linear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5637b055b954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'calculating Validation risk for learning_rate:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and lamda:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlamda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainingData_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainingLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1a6be6f5eccb>\u001b[0m in \u001b[0;36mcalculate_error\u001b[0;34m(a, b, x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainingData_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mk_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculate_k_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_k_linear' is not defined"
     ]
    }
   ],
   "source": [
    "print('calculating Validation risk for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "print(calculate_error(a,b,trainingData_norm,trainingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.00000000001\n",
    "lamda=0.00000001\n",
    "T=50\n",
    "a,b=gradient_descent(T,learning_rate,lamda)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('calculating Validation risk for learning_rate:',learning_rate,'and lamda:',lamda)\n",
    "print(calculate_error(a,b,trainingData_norm,trainingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 different Learning rates\n",
    "#Training and Validation risk for each learning rate\n",
    "print('Training and Validation risk for 54 (learning rates,Lambda) combinations')\n",
    "print('Learning rates:0.000001,0.000005,0.00001,0.00005,0.0001,0.005,0.001,0.01,1')\n",
    "print('Lambda:0.0001,0.001,0.01,0.1,0,1')\n",
    "\n",
    "validation_errors=[]\n",
    "training_errors=[]\n",
    "learning_rates=[]\n",
    "lambdas=[]\n",
    "T=10\n",
    "for learning_rate in [0.000001,0.000005,0.00001,0.00005,0.0001,0.005,0.001,0.01,1]:\n",
    "    for lamda in [0.0001,0.001,0.01,0.1,0,1]:\n",
    "        a,b=gradient_descent(T,learning_rate,lamda)\n",
    "        learning_rates.append(learning_rate)\n",
    "        lambdas.append(lamda)\n",
    "        \n",
    "        validation_error=calculate_error(a,b,validationData_norm,validationLabels)\n",
    "        validation_errors.append(validation_error)\n",
    "        \n",
    "        training_error=calculate_error(a,b,trainingData_norm,trainingLabels)\n",
    "        training_errors.append(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['Learning_rate','Lambda','Training_Error','Validation_Error'])\n",
    "df['Learning_rate']=learning_rates\n",
    "df['Lambda']=lambdas\n",
    "df['Training_Error']=training_errors\n",
    "df['Validation_Error']=validation_errors\n",
    "#display(df)\n",
    "#print \"Dataframe 2:\"\n",
    "HTML(df.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Learning_rate which yields least validation_error\n",
    "minimum_validation_error=min(validation_errors)\n",
    "index_of_minimum=validation_errors.index(minimum_validation_error)\n",
    "learning_rate,lamda=learning_rates[index_of_minimum],lambdas[index_of_minimum]\n",
    "print(\"learning_rate for least validation risk\",learning_rate)\n",
    "print(\"lambda for least validation risk\",lamda)\n",
    "print('Validation Error for learning rate:',learning_rate,'and lambda:',lamda,'is:',minimum_validation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function used to generate list of empirical risk for iterations 1 to T\n",
    "def Empiricial_risk_vs_iterations(T,learning_rate,lamda):\n",
    "    a=np.zeros((len(trainingData_norm),1), dtype=np.float128)\n",
    "    b=0\n",
    "    list_empirical_risk=[]\n",
    "    for t in range(1,T+1):\n",
    "        k_a_b=np.dot(k,a)+b\n",
    "        gradient_a=calculate_gradient_a(a,k_a_b,lamda)\n",
    "        gradient_b=calculate_gradient_b(a,k_a_b,lamda)\n",
    "        a-=(learning_rate*gradient_a)\n",
    "        b-=(learning_rate*gradient_b)\n",
    "        \n",
    "        empirical_risk=calculate_error(a,b,trainingData_norm,trainingLabels)\n",
    "        list_empirical_risk.append(empirical_risk)\n",
    "    return list_empirical_risk\n",
    "\n",
    "#list of empirical risk values, for iterations from 1 to 1000\n",
    "T=500\n",
    "list_empirical_risk=Empiricial_risk_vs_iterations(T,learning_rate,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot Empirical Risk vs Iterations\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,501), list_empirical_risk, 'r--')\n",
    "plt.ylabel('Empiricial risk')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(' Empirical Risk vs Iterations')\n",
    "plt.show()\n",
    "\n",
    "#Find number of iterations which gives minimum risk \n",
    "number_of_iterations=list_empirical_risk.index(min(list_empirical_risk))+1\n",
    "print(\"Iteration which produces least empirical risk:\",number_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Final Model\n",
    "#Best model using the learning rate and number_of_iterations found\n",
    "a,b=gradient_descent(number_of_iterations,learning_rate,lamda)\n",
    "test_error=calculate_error(a,b,testData_norm,testLabels)\n",
    "print(\"best model: learning rate:\",learning_rate,'and lamda:',lamda,' for iterations:',number_of_iterations)\n",
    "print('Test Risk of the best model:',test_error)\n",
    "\n",
    "print('\\nNote:')\n",
    "print('Test Risk:',test_error, 'and Validation Risk:',minimum_validation_error,'are close')\n",
    "print('Empirical Risk reduces as iterations increase as expected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation Accuracy vs Test Accuracy\n",
    "\n",
    "#Function to calculate Risk\n",
    "def calculate_accuracy(a,b,x,y):\n",
    "    x_train=trainingData_norm\n",
    "    N=len(y)\n",
    "    k_=calculate_k(x,x_train)\n",
    "    prediction=np.sum(np.multiply(k_,a),axis=1)+b\n",
    "    prediction[prediction <= 0] = -1.0\n",
    "    prediction[prediction > 0] = 1.0\n",
    "    correct_classification=len([a for a,b in zip(prediction,y) if a==b])\n",
    "    accuracy=correct_classification*100.0/N\n",
    "    return accuracy\n",
    "\n",
    "a,b=gradient_descent(number_of_iterations,learning_rate,lamda)\n",
    "#test_accuracy=calculate_accuracy(w,testData,testLabels)\n",
    "validation_accuracy=calculate_accuracy(a,b,validationData_norm,validationLabels)\n",
    "training_accuracy=calculate_accuracy(a,b,trainingData_norm,trainingLabels)\n",
    "\n",
    "#print('training_accuracy',training_accuracy)\n",
    "print('validation_accuracy',validation_accuracy)\n",
    "print('test_accuracy',test_accuracy)\n",
    "\n",
    "print('Note:')\n",
    "print('Validation Accuracy and Test Accuracy are close')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
